{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tslearn\\clustering\\kmeans.py:16: UserWarning: Scikit-learn <0.24 will be deprecated in a future release of tslearn\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import collections\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from tslearn.clustering import TimeSeriesKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = logging.getLogger(__name__)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "logf = open(\"error.log\", \"w\") #error log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DESCRIPTION:\n",
    "Loading data from the csv to dataframe. Only 3 columns are kept \n",
    "for initial conderstanding: entry_date, rounded_time,price\n",
    "\n",
    "@Input: \n",
    "    filepath: file containing electricity usage\n",
    "\n",
    "@Output: dataframe containing all entries of input file for 3 columns\n",
    "\n",
    "'''\n",
    "\n",
    "def data_load(filepath):\n",
    "    try:\n",
    "        logging.info('Loading data from file')\n",
    "        \n",
    "        data = pd.read_csv(filepath)\n",
    "        data = data[['entry_date','rounded_time','price']]\n",
    "        data['entry_date'] = pd.to_datetime(data['entry_date'])\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logf.write(\"Failed to load the data from {0}: {1}\\n\".format(str(filepath), str(e)))\n",
    "        LOG.error(traceback.format_exc())\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DESCRIPTION:\n",
    "To bring all prices of current data in same scale\n",
    "\n",
    "@Input:\n",
    "    data: dataframe\n",
    "\n",
    "@Ouput: normalized prices in the dataframe\n",
    "\n",
    "'''\n",
    "\n",
    "def normalize_df(data):\n",
    "    try:\n",
    "        return data.apply(lambda s: s / data.max(axis=1)).fillna(0)\n",
    "    \n",
    "    except Exception:\n",
    "        logf.write(\"Failed to normalized the data {0}\\n\".str(e))\n",
    "        LOG.error(traceback.format_exc())\n",
    "        sys.exit(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DESCRIPTION:\n",
    "Converting the data in the pivot table format to manipulate it as time series.\n",
    "Selecting 6 months data from the start date. \n",
    "Returning normalized data\n",
    "\n",
    "@Input: \n",
    "    data: dataframe containing complete data\n",
    "    start_date, end_date: date range of the data    \n",
    "\n",
    "@Output: 6 months- (normalized data, pivoted data )\n",
    "\n",
    "'''\n",
    "\n",
    "def data_preprocessing(data, start_date, end_date):\n",
    "    \n",
    "    try:\n",
    "        #Selecting the range i.e. 6 months\n",
    "        ranged_data =  data.loc[(data['entry_date'] >= start_date) & (data['entry_date'] < end_date)]\n",
    "        pivoted = ranged_data.pivot_table(index=\"entry_date\", columns=\"rounded_time\", values=\"price\", fill_value=0)\n",
    "\n",
    "        #normalizing the data\n",
    "        data_norm = (normalize_df(pivoted.loc[:, pivoted.columns.difference([\"rounded_time\"])]).round(3))\n",
    "        return pivoted, data_norm\n",
    "    \n",
    "    except Exception:\n",
    "        logf.write(\"Failed in pre-processing the data {0}\\n\".str(e))\n",
    "        LOG.error(traceback.format_exc())\n",
    "        sys.exit(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DESCRIPTION: \n",
    "Plots cluster centroids.\n",
    "\n",
    "@Input: \n",
    "    data: dataframe containing 6 months of data\n",
    "    clust: centroid for the data\n",
    "\n",
    "'''\n",
    "def plot_cluster_centroids(data, clust, lw=4, alpha=0.6):\n",
    "    try:\n",
    "        fontsize = 15\n",
    "        data.assign(clust=clust).groupby(\"clust\").mean().T.plot(ax=plt.gca(), lw=lw, alpha=alpha)\n",
    "\n",
    "        plt.title(\"Cluster Centroids\", fontsize=fontsize+5)\n",
    "        plt.yticks(fontsize=fontsize);\n",
    "        plt.legend(title=\"Cluster centroids:\", loc=\"upper left\")\n",
    "        plt.grid()\n",
    "    \n",
    "    except Exception as E:\n",
    "        logf.write(\"Failed in plotting cluster centroid of the the data {0}\\n\".str(e))\n",
    "        LOG.error(traceback.format_exc())\n",
    "        sys.exit(1)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DESCRIPTION:\n",
    "Plot one subplot per cluster along with cluster centroid\n",
    "in decreasing order of cluster size percentage\n",
    "\n",
    "'''\n",
    "def plot_clustered_profiles(df, clust, n_cols=3, alpha=0.2):\n",
    "    try:\n",
    "        weekly = False if df.shape[1] < 168 else True\n",
    "\n",
    "        #calculating cluster size\n",
    "        clust_perc = 100 * clust.value_counts(normalize=True)\n",
    "\n",
    "        n_rows = np.ceil(clust.nunique() / n_cols)\n",
    "        fontsize = 15\n",
    "        fig = plt.figure(figsize=[15, n_rows*4])\n",
    "\n",
    "        for i, clust_n in enumerate(clust_perc.index):\n",
    "            ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "            df_plot = df[clust == clust_n]\n",
    "\n",
    "            step = 10 if df_plot.shape[0] > 500 else 1  # plot less profiles\n",
    "\n",
    "            plt.plot(df_plot.iloc[::step].T.values, alpha=alpha, color=\"red\")\n",
    "            df_plot.mean().plot(ax=plt.gca(), alpha=1, color=\"k\", legend=False);\n",
    "\n",
    "            plt.title(\"clust: {}, perc: {:.1f}%\".format(clust_n, \n",
    "                                                        clust_perc.loc[clust_n]), \n",
    "                                                        fontsize=fontsize+5);\n",
    "            plt.yticks(fontsize=fontsize);\n",
    "            plt.grid()\n",
    "\n",
    "        plt.tight_layout()     \n",
    "    except Exception as e:\n",
    "        logf.write(\"Failed in ploting clustered data{0}\\n\".str(e))\n",
    "        LOG.error(traceback.format_exc())\n",
    "        sys.exit(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DESCRIPTION: Applying KMeans on 6 months data\n",
    "\n",
    "@Input: data to be clustered\n",
    "\n",
    "@Output: clustersed data\n",
    "\n",
    "'''\n",
    "def train(X):\n",
    "    try:\n",
    "        logging.info('Clustering data')\n",
    "        \n",
    "        model = TimeSeriesKMeans(n_clusters=6, metric=\"softdtw\", max_iter=10, random_state= 20).fit(X)\n",
    "        clusters = pd.Series(model.labels_, index=X.index)    \n",
    "        return clusters\n",
    "    except Exception as e:\n",
    "        logf.write(\"Failed in clustering data{0}\\n\".str(e))\n",
    "        LOG.error(traceback.format_exc())\n",
    "        sys.exit(1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "1. In every iteration 6 months data is been considered, based on start date and end date.\n",
    "\n",
    "2. After preprocessing data, this 6 month data is divided in 6 clusters (as mentioned in the problem statement)\n",
    "\n",
    "3. Clustering is done for 26 iterations after shifting the data of previous iteration by one week. To understand the stability, cluster has to be mapped across every iteration. \n",
    "    * **To perform which cluster in current iteration is same as cluster in previous iteration, I considered the size of every cluster in adjacent iterations. Clusters which has largest size in $iteration1$ will be same as cluster having same rank in size in $iteration2$**. Clusters in every iteration is sorted on the basis of size and largest cluster of iteration1 is mapped to largest cluster of iteration2, second largest cluster of iteration1 is mapped to second largest cluster of iteration2, so on and so forth. \n",
    "    * This is maintained in a dictionary(name=mapping) as a list of list. Key value of this dictionary indicates the size rank i.e. key = 0 holds value of cluster number and centroid having largest size in every iteration, key =1 holds value of cluster number and centroid having second largest size in every iteration.\n",
    "    \n",
    "    **Example**: \n",
    "    \n",
    "    mapping = {\"0\" : [[$iteration1$ largest  size cluster number, centroid],[$iteration2$ largest size cluster number, centroid} ]....[$iteration26$ largest size cluster number, centroid ]],     \n",
    "    \n",
    "    \"1\": [[$iteration1$ second largest size cluster number, centroid ],[$iteration2$ second largest size cluster number, centroid ].....[$iteration26$ second largest size cluster number, centroid]], \n",
    "    ..... }\n",
    "    \n",
    "   <br>     \n",
    "    * After atleast one shift is completed, to calculate the stability/movement **euclidean distance** is calculated between cluster centroid of adjacent iteration belonging to same rank in size or same key of \"mapping\" dictionary. These values are stored in another dictionary where key again represent rank of cluster in size (0 means largest size). This distance combines complete movement between 2 same cluster when shifted by $n$ number of weeks from starting date.\n",
    "\n",
    "\n",
    "4. Now we have distance for all 6 clusters based on size. To map these distance with cluster number, for very cluster I considered its rank as per first iteration and mapped it to distance values stored in the \"movement\" dictionary.\n",
    "\n",
    "\n",
    "5. Output dictionary holds the movement for a very cluster from $iteration1$ to $iteration26$. More the value for a cluster, more is the change in the cluster structure after 26 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DESCRIPTION: Creating the mapping between clusters size across iteration and movement\n",
    "@Input: Complete data\n",
    "\n",
    "'''\n",
    "def rank(data):\n",
    "\n",
    "    mapping = collections.defaultdict(list) #adding mapping between cluster number and its centroid  after every iteration\n",
    "    movement = collections.defaultdict(float) #storing movement of cluster based on size\n",
    "    iteration = 26\n",
    "    \n",
    "    try:\n",
    "        first_date = data['entry_date'][0] #date of first entry in the data file\n",
    "\n",
    "        for itrt in range(iteration):\n",
    "\n",
    "            #shifting data by 1 week\n",
    "            week = datetime.timedelta(days=7*itrt)\n",
    "            start_date = first_date + week\n",
    "            end_date = start_date + pd.tseries.offsets.DateOffset(months=6) #end date of 6 months from start date\n",
    "\n",
    "            # converting date as per data in csv\n",
    "            end_date = datetime.datetime.strptime(str(end_date), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            #processing 6 months data\n",
    "            pivoted,preprocessed_data = data_preprocessing(data,start_date,end_date)\n",
    "\n",
    "            #creating clusters for every 6 month data\n",
    "            clusters = train(preprocessed_data)\n",
    "\n",
    "            #To plot all subplot for every iteration and centroid. \n",
    "            # To save computation time, below line is commented\n",
    "            # Sample plot is shown below\n",
    "            #plot_clustered_profiles(pivoted, clusters) \n",
    "            #plot_cluster_centroids(preprocessed_data, clusters)\n",
    "\n",
    "\n",
    "            # Associating cluster number and percentage size of corresponding cluster in decreasing order, \n",
    "            # format: \"cluster number percentage size\"\n",
    "            clust_perc = 100 * clusters.value_counts(normalize=True)\n",
    "\n",
    "            # identify centroid value for every cluster. \n",
    "            # Centroid will be a time series, respresenting all time series of that cluster.\n",
    "            centroid_list = preprocessed_data.assign(clusters=clusters).groupby(\"clusters\").mean()\n",
    "\n",
    "            cluster_num = clust_perc.index.values # list of cluster numbers in descending order based on the size.\n",
    "\n",
    "            for index,clust_num in enumerate(cluster_num):\n",
    "                centroid = centroid_list.iloc[clust_num] # storing centroid of a particular cluster number\n",
    "                mapping[index].append([clust_num,centroid]) #index =0 hold clusters of max size, index=1 clusters of second largest size\n",
    "\n",
    "                #After data is shifted alteast once by one week\n",
    "                if itrt > 0:\n",
    "                    prev_iteration = mapping[index][-1][1]\n",
    "                    cur_iteration = mapping[index][-2][1]\n",
    "\n",
    "                    #taking euclidean distance between 2 cluster centroid which are mapping same in two adjacent iterations\n",
    "                    distance = np.linalg.norm(prev_iteration-cur_iteration)\n",
    "                    movement[index] += distance #storing value of difference between 2 clusters considered as same as data shift\n",
    "            \n",
    "        return mapping,movement\n",
    "        \n",
    "    except Exception as e:\n",
    "        logf.write(\"Failed calculating stability of data{0}\\n\".str(e))\n",
    "        LOG.error(traceback.format_exc())\n",
    "        sys.exit(1)    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping complete movement/shift value across 26 iterations\n",
    "# with it's corresponding cluster value\n",
    "\n",
    "def movement(mapping,movement):\n",
    "    try:\n",
    "        output = {} #key = cluster number, value = complete movement from iteration 1 till 26\n",
    "        for pos,key in enumerate(mapping):\n",
    "            output[mapping[key][0][0]] = movement[pos]\n",
    "    \n",
    "        return(output)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logf.write(\"Failed to create mapping for cluster number and movement{0}\\n\".str(e))\n",
    "        LOG.error(traceback.format_exc())\n",
    "        sys.exit(1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.9347173669873775, 4: 30.3392098804596, 2: 132.09642852564184, 1: 98.06706371677495, 3: 169.27808595196987, 5: 0.0}\n"
     ]
    }
   ],
   "source": [
    "data = data_load('Downloads/dataset_dollarhide.csv')\n",
    "mapping,distance = rank(data)\n",
    "output = movement(mapping,distance)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "#### Clustered data for one iteration\n",
    "\n",
    "<img src=\"/clustered_data.png\">\n",
    "\n",
    "#### Centroid for one iteration\n",
    "\n",
    "<img src=\"/centroid.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other approaches which I tried to track clusters across iteration**:\n",
    "    \n",
    "   **Appproach 1**: In very iteration storing the centroid and calculate the difference of every cluster of $iteration1$ with every cluster $iteraion2$, but this was mapping all cluster with same cluster of next adjacent iteration.\n",
    "    \n",
    "   **Appproach 2**: Train KMeans model on one iteration, store it. Train model on next iteration. Identify the centroid and store them. Using these centroid and previous adjacent iteration model to predict cluster number. It has the same problem as approach 1.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future Enhancement:**\n",
    "\n",
    "1. Using multiprocessing to run create seperate process for different 6 months data based on CPU/GPU core\n",
    "2. Using LSTM to better understand timeseries data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
